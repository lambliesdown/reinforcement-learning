{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation in Cliff Walking Environment - TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cliff Walking Environment\n",
    "\n",
    "The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.\n",
    "\n",
    "The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.\n",
    "\n",
    "<img src=\"cliffworld.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "0d8193313c9cea5606856433732df077",
     "grade": false,
     "grade_id": "cell-917f710997077ab6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from rllib.rl_glue import RLGlue\n",
    "from rllib.environment.cliff_walk_environment import CliffWalkEnvironment\n",
    "from rllib.agent.td_agent import TDAgent\n",
    "import numpy as np\n",
    "from operator import add\n",
    "from manager import Manager\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    " \n",
    "Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).\n",
    "\n",
    "<img src=\"cliffworld-annotated.png\" style=\"height:400px\">\n",
    "\n",
    "Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    coords_to_test = [(0, 0), (0, 11), (1, 5), (3, 0), (3, 9), (3, 11)]\n",
    "    true_states = [0, 11, 17, 36, 45, 47]\n",
    "    output_states = [env.state(coords) for coords in coords_to_test]\n",
    "    assert(output_states == true_states)\n",
    "test_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)\n",
    "# NOTE: The test below is again limited in scope. Hidden tests are used in the autograder.\n",
    "#       You may wish to run other tests to check your implementation.\n",
    "def test_action_up():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    env.agent_loc = (0, 0)\n",
    "    env.env_step(0)\n",
    "    assert(env.agent_loc == (0, 0))\n",
    "    \n",
    "    env.agent_loc = (1, 0)\n",
    "    env.env_step(0)\n",
    "    assert(env.agent_loc == (0, 0))\n",
    "test_action_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    env.agent_loc = (0, 0)\n",
    "    reward_state_term = env.env_step(0)\n",
    "    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((0, 0)) and\n",
    "           reward_state_term[2] == False)\n",
    "    \n",
    "    env.agent_loc = (3, 1)\n",
    "    reward_state_term = env.env_step(2)\n",
    "    assert(reward_state_term[0] == -100 and reward_state_term[1] == env.state((3, 0)) and\n",
    "           reward_state_term[2] == False)\n",
    "    \n",
    "    env.agent_loc = (2, 11)\n",
    "    reward_state_term = env.env_step(2)\n",
    "    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((3, 11)) and\n",
    "           reward_state_term[2] == True)\n",
    "test_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_td_updates():\n",
    "    # The following test checks that the TD check works for a case where the transition \n",
    "    # garners reward -1 and does not lead to a terminal state. This is in a simple two state setting \n",
    "    # where there is only one action. The first state's current value estimate is 0 while the second is 1.\n",
    "    # Note the discount and step size if you are debugging this test.\n",
    "    agent = TDAgent()\n",
    "    policy_list = np.array([[1.], [1.]])\n",
    "    agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n",
    "    agent.values = np.array([0., 1.])\n",
    "    agent.agent_start(0)\n",
    "    reward = -1\n",
    "    next_state = 1\n",
    "    agent.agent_step(reward, next_state)\n",
    "    assert(np.isclose(agent.values[0], -0.001) and np.isclose(agent.values[1], 1.))\n",
    "    \n",
    "    # The following test checks that the TD check works for a case where the transition \n",
    "    # garners reward -100 and lead to a terminal state. This is in a simple one state setting \n",
    "    # where there is only one action. The state's current value estimate is 0.\n",
    "    # Note the discount and step size if you are debugging this test.\n",
    "    agent = TDAgent()\n",
    "    policy_list = np.array([[1.]])\n",
    "    agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n",
    "    agent.values = np.array([0.])\n",
    "    agent.agent_start(0)\n",
    "    reward = -100\n",
    "    next_state = 0\n",
    "    agent.agent_end(reward)\n",
    "    assert(np.isclose(agent.values[0], -10))\n",
    "    \n",
    "test_td_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation Experiments\n",
    "\n",
    "Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.\n",
    "\n",
    "The code below runs one run of an experiment given env_info and agent_info dictionaries. A \"manager\" object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def run_experiment(env_info, agent_info, \n",
    "                   num_episodes=5000,\n",
    "                   experiment_name=None,\n",
    "                   plot_freq=100,\n",
    "                   true_values_file=None,\n",
    "                   value_error_threshold=1e-8):\n",
    "    env = CliffWalkEnvironment\n",
    "    agent = TDAgent\n",
    "    rl_glue = RLGlue(env, agent)\n",
    "\n",
    "    rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        rl_glue.rl_episode(0) # no step limit\n",
    "        if episode % plot_freq == 0:\n",
    "            values = rl_glue.agent.agent_message(\"get_values\")\n",
    "            manager.visualize(values, episode)\n",
    "\n",
    "    values = rl_glue.agent.agent_message(\"get_values\")\n",
    "    if true_values_file is not None:\n",
    "        # Grading: The Manager will check that the values computed using your TD agent match \n",
    "        # the true values (within some small allowance) across the states. In addition, it also\n",
    "        # checks whether the root mean squared value error is close to 0.\n",
    "        manager.run_tests(values, value_error_threshold)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fad676a6e39768fdfe6897b2fb9032a",
     "grade": false,
     "grade_id": "cell-17e3e97164635860",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_info = {\"grid_height\": 4, \"grid_width\": 12, \"seed\": 0}\n",
    "agent_info = {\"discount\": 1, \"step_size\": 0.01, \"seed\": 0}\n",
    "\n",
    "# The Optimal Policy that strides just along the cliff\n",
    "policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n",
    "policy[36] = [1, 0, 0, 0]\n",
    "for i in range(24, 35):\n",
    "    policy[i] = [0, 0, 0, 1]\n",
    "policy[35] = [0, 0, 1, 0]\n",
    "\n",
    "agent_info.update({\"policy\": policy})\n",
    "\n",
    "true_values_file = \"optimal_policy_value_fn.npy\"\n",
    "_ = run_experiment(env_info, agent_info, num_episodes=5000, experiment_name=\"Policy Evaluation on Optimal Policy\",\n",
    "                   plot_freq=500, true_values_file=true_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Safe Policy\n",
    "# Hint: Fill in the dictionary below (as done in the previous cell) based on the safe policy illustration \n",
    "# in the environment diagram. This is the policy that strides as far as possible away from the cliff. \n",
    "# We call it a \"safe\" policy because if the environment has any stochasticity, this policy would do a good job in \n",
    "# keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). \n",
    "# BOILERPLATE:\n",
    "policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n",
    "\n",
    "w = env_info['grid_width']\n",
    "h = env_info['grid_height']\n",
    "\n",
    "for i in range(w-1):\n",
    "    policy[i] = [0,0,0,1]\n",
    "    \n",
    "for i in range(1,h):\n",
    "    policy[w*i] = [1,0,0,0]\n",
    "    policy[(w-1)+w*(i-1)] = [0,0,1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent_info.update({\"policy\": policy})\n",
    "v = run_experiment(env_info, agent_info,\n",
    "               experiment_name=\"Policy Evaluation On Safe Policy\",\n",
    "               num_episodes=5000, plot_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Near Optimal Stochastic Policy\n",
    "# Now, we try a stochastic policy that deviates a little from the optimal policy seen above. \n",
    "# This means we can get different results due to randomness.\n",
    "# We will thus average the value function estimates we get over multiple runs. \n",
    "# This can take some time, upto about 5 minutes from previous testing. \n",
    "\n",
    "env_info = {\"grid_height\": 4, \"grid_width\": 12}\n",
    "agent_info = {\"discount\": 1, \"step_size\": 0.01}\n",
    "\n",
    "policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n",
    "policy[36] = [0.9, 0.1/3., 0.1/3., 0.1/3.]\n",
    "for i in range(24, 35):\n",
    "    policy[i] = [0.1/3., 0.1/3., 0.1/3., 0.9]\n",
    "policy[35] = [0.1/3., 0.1/3., 0.9, 0.1/3.]\n",
    "agent_info.update({\"policy\": policy})\n",
    "agent_info.update({\"step_size\": 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(30)):\n",
    "    env_info['seed'] = i\n",
    "    agent_info['seed'] = i\n",
    "    v = run_experiment(env_info, agent_info,\n",
    "                   experiment_name=\"Policy Evaluation On Optimal Policy\",\n",
    "                   num_episodes=5000, plot_freq=10000)\n",
    "    arr.append(v)\n",
    "average_v = np.array(arr).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "74d80436e59bbf99333411fd5ac1675b",
     "grade": false,
     "grade_id": "cell-0276c863cb27066d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Wrapping Up\n",
    "Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "sample-based-learning-methods",
   "graded_item_id": "P4k5f",
   "launcher_item_id": "OwIbv"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
