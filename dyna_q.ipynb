{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q and Dyna-Q+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee25bdba2a442f7c9bb323036ff7adff",
     "grade": false,
     "grade_id": "cell-6ab578539f713801",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Welcome to this programming assignment! In this notebook, you will:\n",
    "1. implement the Dyna-Q and Dyna-Q+ algorithms. \n",
    "2. compare their performance on an environment which changes to become 'better' than it was before, that is, the task becomes easier. \n",
    "\n",
    "We will give you the environment and infrastructure to run the experiment and visualize the performance. The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness. \n",
    "\n",
    "Please go through the cells in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Shortcut Maze Environment\n",
    "\n",
    "In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions – up, down, right, left – which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with $\\gamma = 0.95$.\n",
    "\n",
    "<img src=\"./images/shortcut_env.png\" alt=\"environment\" width=\"400\"/>\n",
    "\n",
    "Later in the assignment, we will use a variant of this maze in which a 'shortcut' opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from rllib.agent.dyna_q_agent import DynaQAgent\n",
    "from rllib.agent.dyna_q_plus_agent import DynaQPlusAgent\n",
    "from rllib.environment.maze_environment import ShortcutMazeEnvironment\n",
    "from rllib.rl_glue import RLGlue\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a42e5b7f720fd0165a2b12b4c99f164",
     "grade": false,
     "grade_id": "cell-70ba6356f71f04d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [8,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "663ef3e7cbb35c2f7bcc99e48697e20c",
     "grade": false,
     "grade_id": "cell-ae016536341366d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's start with a quick recap of the tabular Dyna-Q algorithm.\n",
    "\n",
    "<div style=\"width:80%\"><img src=\"./images/DynaQ.png\" alt=\"DynaQ_pseudocode\"></div>\n",
    "\n",
    "Dyna-Q involves four basic steps:\n",
    "1. Action selection: given an observation, select an action to be performed (here, using the $\\epsilon$-greedy method).\n",
    "2. Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning).\n",
    "3. Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).\n",
    "4. Planning: update the action values by generating $n$ simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the 'Indirect RL' step. The process of choosing the state and action to simulate an experience with is known as 'search control'.\n",
    "\n",
    "Steps 1 and 2 are parts of the [tabular Q-learning algorithm](http://www.incompleteideas.net/book/RLbook2018.pdf#page=153) and are denoted by line numbers (a)–(d) in the pseudocode above. Step 3 is performed in line (e), and Step 4 in the block of lines (f).\n",
    "\n",
    "We highly recommend revising the Dyna videos in the course and the material in the RL textbook (in particular, [Section 8.2](http://www.incompleteideas.net/book/RLbook2018.pdf#page=183))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a67870254bf6049b72911dcbe495ad84",
     "grade": false,
     "grade_id": "cell-feda394cc8d0d0f0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `update_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f4aeb91268cae5d6169083963dd4460",
     "grade": true,
     "grade_id": "DynaQ_update_model",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "## Test code for update_model() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,0,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,1,2)\n",
    "print(\"Model: \\n\", test_agent.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "58a644632b6aea7d2850370ab1066b9e",
     "grade": false,
     "grade_id": "cell-47eee79fcc885cb6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Model: \n",
    " {0: {2: (0, 1), 3: (1, 2)}, 2: {0: (1, 1)}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "022cff5daeb2a0885bec5f66231b0233",
     "grade": false,
     "grade_id": "cell-deb5f5adef22b4e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `planning_step()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for planning_step() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 4,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 5}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,1,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,0,1)\n",
    "test_agent.update_model(0,1,-1,1)\n",
    "test_agent.planning_step()\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cafc07b98a92e3fd29e912aca8a1d2d3",
     "grade": false,
     "grade_id": "cell-2b479d946144873d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Model: \n",
    " {0: {2: (1, 1), 3: (0, 1), 1: (-1, 1)}, 2: {0: (1, 1)}}\n",
    "Action-value estimates: \n",
    " [[0.   0.1   0.   0.2 ]\n",
    " [0.   0.   0.   0.  ]\n",
    " [0.1 0.   0.   0.  ]]\n",
    "```\n",
    "\n",
    "If your output does not match the above, one of the first things to check is to make sure that you haven't changed the `planning_random_seed` in the test cell. Additionally, make sure you have handled terminal updates correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `agent_start()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_start() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "action = test_agent.agent_start(0)\n",
    "print(\"Action:\", action)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2c06652b1c989ff3174e42a463173ac",
     "grade": false,
     "grade_id": "cell-bc7046affcf9c2f9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Action: 1\n",
    "Model: \n",
    " {}\n",
    "Action-value estimates: \n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b5b76c8ebc0936d8ca8b929d1721fe44",
     "grade": false,
     "grade_id": "cell-069a254ee4ba6e25",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `agent_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_step() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 2,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions.append(test_agent.agent_start(0))\n",
    "actions.append(test_agent.agent_step(1,2))\n",
    "actions.append(test_agent.agent_step(0,1))\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a47eae38f5b82056b69ea71037a76465",
     "grade": false,
     "grade_id": "cell-0b8605acd440fc7d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Actions: [1, 3, 1]\n",
    "Model: \n",
    " {0: {1: (2, 1)}, 2: {3: (1, 0)}}\n",
    "Action-value estimates: \n",
    " [[0.     0.3439 0.     0.    ]\n",
    " [0.     0.     0.     0.    ]\n",
    " [0.     0.     0.     0.    ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1fee78afea91645becfd5b193b4b9ab",
     "grade": false,
     "grade_id": "cell-8d3e06f7f489a49c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `agent_end()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_end() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 2,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions.append(test_agent.agent_start(0))\n",
    "actions.append(test_agent.agent_step(1,2))\n",
    "actions.append(test_agent.agent_step(0,1))\n",
    "test_agent.agent_end(1)\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value Estimates: \\n\", test_agent.q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43711ecd45a75f4ee1f31ac3f5127477",
     "grade": false,
     "grade_id": "cell-25bdfd8dc303b1e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Actions: [1, 3, 1]\n",
    "Model: \n",
    " {0: {1: (2, 1)}, 2: {3: (1, 0)}, 1: {1: (-1, 1)}}\n",
    "Action-value Estimates: \n",
    " [[0.      0.41051 0.      0.     ]\n",
    " [0.      0.1     0.      0.     ]\n",
    " [0.      0.      0.      0.01   ]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "12f59fa0e03a4b5df596a73e3dae0c02",
     "grade": false,
     "grade_id": "cell-329423dc3230312d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Experiment: Dyna-Q agent in the maze environment\n",
    "\n",
    "Alright. Now we have all the components of the `DynaQAgent` ready. Let's try it out on the maze environment! \n",
    "\n",
    "The next cell runs an experiment on this maze environment to test your implementation. The initial action values are $0$, the step-size parameter is $0.125$. and the exploration parameter is $\\epsilon=0.1$. After the experiment, the sum of rewards in each episode should match the correct result.\n",
    "\n",
    "We will try planning steps of $0,5,50$ and compare their performance in terms of the average number of steps taken to reach the goal state in the aforementioned maze environment. For scientific rigor, we will run each experiment $30$ times. In each experiment, we set the initial random-number-generator (RNG) seeds for a fair comparison across algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2573b31bd8cce3a89beba2ec09ab2c3",
     "grade": false,
     "grade_id": "cell-28355ff7447c705f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_episodes = exp_parameters['num_episodes']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = env_parameters                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  # We pass the agent the information it needs. \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) # for collecting metrics \n",
    "    log_data = {'planning_steps_all' : planning_steps_all}                     # that shall be plotted later\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 0.5')                    # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  \n",
    "\n",
    "        for i in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['seed'] = i\n",
    "            agent_info['planning_seed'] = i\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            for j in range(num_episodes):\n",
    "\n",
    "                rl_glue.rl_start()                # We start an episode. Here we aren't using rl_glue.rl_episode()\n",
    "                                                  # like the other assessments because we'll be requiring some \n",
    "                is_terminal = False               # data from within the episodes in some of the experiments here \n",
    "                num_steps = 0\n",
    "                while not is_terminal:\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step \n",
    "                    num_steps += 1                                      # and return the reward and action taken.\n",
    "\n",
    "                all_averages[idx][i][j] = num_steps\n",
    "\n",
    "    log_data['all_averages'] = all_averages\n",
    "    np.save(\"results/Dyna-Q_planning_steps\", log_data)\n",
    "    \n",
    "\n",
    "def plot_steps_per_episode(file_path):\n",
    "\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    all_averages = data['all_averages']\n",
    "    planning_steps_all = data['planning_steps_all']\n",
    "\n",
    "    for i, planning_steps in enumerate(planning_steps_all):\n",
    "        plt.plot(np.mean(all_averages[i], axis=0), label='Planning steps = '+str(planning_steps))\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
    "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT modify the parameter settings.\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 30,                     # The number of times we run the experiment\n",
    "    \"num_episodes\" : 40,                 # The number of episodes per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"planning_steps\" : [0, 5, 50]       # The list of planning_steps we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQAgent              # The agent\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "plot_steps_per_episode('results/Dyna-Q_planning_steps.npy')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b78255378e333ad6a4b69eadb1c5bce9",
     "grade": false,
     "grade_id": "cell-e55bf393c9e5a94b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do you notice?\n",
    "\n",
    "As the number of planning steps increases, the number of episodes taken to reach the goal decreases rapidly. Remember that the RNG seed was set the same for all the three values of planning steps, resulting in the same number of steps taken to reach the goal in the first episode. Thereafter, the performance improves. The slowest improvement is when there are $n=0$ planning steps, i.e., for the non-planning Q-learning agent, even though the step size parameter was optimized for it. Note that the grey dotted line shows the minimum number of steps required to reach the goal state under the optimal greedy policy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38982e501241998792ae2aebe105c47b",
     "grade": false,
     "grade_id": "cell-56f6a9492acc5115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Experiment(s): Dyna-Q agent in the _changing_ maze environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e53909e26c756c30000e1a0f0d9b4c",
     "grade": false,
     "grade_id": "cell-64cbd79abcf74fce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Great! Now let us see how Dyna-Q performs on the version of the maze in which a shorter path opens up after 3000 steps. The rest of the transition and reward dynamics remain the same. \n",
    "\n",
    "<img src=\"./images/shortcut_env_after.png\" alt=\"environment\" width=\"800\"/>\n",
    "\n",
    "Before you proceed, take a moment to think about what you expect to see. Will Dyna-Q find the new, shorter path to the goal? If so, why? If not, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fd85c37c0082135d539e4160d4e07949",
     "grade": false,
     "grade_id": "cell-8f6730285bc8288e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment_with_state_visitations(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_max_steps = exp_parameters['num_max_steps']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, 54))  # For saving the number of\n",
    "    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, 54))   #     state-visitations \n",
    "    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   # For saving the cumulative reward\n",
    "    log_data = {'planning_steps_all' : planning_steps_all}\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  # We pass the agent the information it needs. \n",
    "\n",
    "        for run in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['seed'] = run\n",
    "            agent_info['planning_seed'] = run\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            num_steps = 0\n",
    "            cum_reward = 0\n",
    "\n",
    "            while num_steps < num_max_steps-1 :\n",
    "\n",
    "                state, _ = rl_glue.rl_start()  # We start the experiment. We'll be collecting the \n",
    "                is_terminal = False            # state-visitation counts to visiualize the learned policy\n",
    "                if num_steps < env_parameters[\"change_at_n\"]: \n",
    "                    state_visits_before_change[idx][run][state] += 1\n",
    "                else:\n",
    "                    state_visits_after_change[idx][run][state] += 1\n",
    "\n",
    "                while not is_terminal and num_steps < num_max_steps-1 :\n",
    "                    reward, state, action, is_terminal = rl_glue.rl_step()  \n",
    "                    num_steps += 1\n",
    "                    cum_reward += reward\n",
    "                    cum_reward_all[idx][run][num_steps] = cum_reward\n",
    "                    if num_steps < env_parameters[\"change_at_n\"]:\n",
    "                        state_visits_before_change[idx][run][state] += 1\n",
    "                    else:\n",
    "                        state_visits_after_change[idx][run][state] += 1\n",
    "\n",
    "    log_data['state_visits_before'] = state_visits_before_change\n",
    "    log_data['state_visits_after'] = state_visits_after_change\n",
    "    log_data['cum_reward_all'] = cum_reward_all\n",
    "    np.save(\"results/\" + result_file_name, log_data)\n",
    "\n",
    "def plot_cumulative_reward(file_path, item_key, y_key, y_axis_label, legend_prefix, title):\n",
    "\n",
    "    data_all = np.load(file_path, allow_pickle=True).item()\n",
    "    data_y_all = data_all[y_key]\n",
    "    items = data_all[item_key]\n",
    "\n",
    "    for i, item in enumerate(items):\n",
    "        plt.plot(np.mean(data_y_all[i], axis=0), label=legend_prefix+str(item))\n",
    "\n",
    "    plt.axvline(x=3000, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel(y_axis_label, rotation=0, labelpad=60)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e463255ee451c8ca255ed3f24dd96bf4",
     "grade": false,
     "grade_id": "cell-7a4965729e7c41f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Did you notice that the environment changes after a fixed number of _steps_ and not episodes? \n",
    "\n",
    "This is because the environment is separate from the agent, and the environment changes irrespective of the length of each episode (i.e., the number of environmental interactions per episode) that the agent perceives. And hence we are now plotting the data per step or interaction of the agent and the environment, in order to comfortably see the differences in the behaviours of the agents before and after the environment changes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5a10d7810edcc38ed364530e320e5b7",
     "grade": false,
     "grade_id": "cell-1585cb7119e3b66d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Okay, now we will first plot the cumulative reward obtained by the agent per interaction with the environment, averaged over 10 runs of the experiment on this changing world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 10,                     # The number of times we run the experiment\n",
    "    \"num_max_steps\" : 6000,              # The number of steps per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "    \"change_at_n\": 3000\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"planning_steps\" : [5, 10, 50]      # The list of planning_steps we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQAgent              # The agent\n",
    "\n",
    "run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, \"Dyna-Q_shortcut_steps\")    \n",
    "plot_cumulative_reward('results/Dyna-Q_shortcut_steps.npy', 'planning_steps_all', 'cum_reward_all', 'Cumulative\\nreward', 'Planning steps = ', 'Dyna-Q : Varying planning_steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ce0809c6896af47bf2391c654533a42",
     "grade": false,
     "grade_id": "cell-bdab2f4622d3890b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We observe that the slope of the curves is almost constant. If the agent had discovered the shortcut and begun using it, we would expect to see an increase in the slope of the curves towards the later stages of training. This is because the agent can get to the goal state faster and get the positive reward. Note that the timestep at which the shortcut opens up is marked by the grey dotted line.\n",
    "\n",
    "Note that this trend is constant across the increasing number of planning steps.\n",
    "\n",
    "Now let's check the heatmap of the state visitations of the agent with `planning_steps=10` during training, before and after the shortcut opens up after 3000 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_visitations(file_path, plot_titles, idx):\n",
    "\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    data_keys = [\"state_visits_before\", \"state_visits_after\"]\n",
    "    positions = [211,212]\n",
    "    titles = plot_titles\n",
    "    wall_ends = [None,-1]\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        state_visits = data[data_keys[i]][idx]\n",
    "        average_state_visits = np.mean(state_visits, axis=0)\n",
    "        grid_state_visits = np.rot90(average_state_visits.reshape((6,9)).T)\n",
    "        grid_state_visits[2,1:wall_ends[i]] = np.nan # walls\n",
    "        #print(average_state_visits.reshape((6,9)))\n",
    "        plt.subplot(positions[i])\n",
    "        plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=1, cmap='viridis')\n",
    "        plt.text(3+0.5, 0+0.5, 'S', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.text(8+0.5, 5+0.5, 'G', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "        cm = plt.get_cmap()\n",
    "        cm.set_bad('gray')\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n",
    "    cax = plt.axes([1., 0.0, 0.075, 1.])\n",
    "    cbar = plt.colorbar(cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_visitations(\"results/Dyna-Q_shortcut_steps.npy\", ['Dyna-Q : State visitations before the env changes', 'Dyna-Q : State visitations after the env changes'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15aded4269ccf83bd3fa9b4ca9be353c",
     "grade": false,
     "grade_id": "cell-61bd2cfdba9cc49d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do you observe?\n",
    "\n",
    "The state visitation map looks almost the same before and after the shortcut opens. This means that the Dyna-Q agent hasn't quite discovered and started exploiting the new shortcut.\n",
    "\n",
    "Now let's try increasing the exploration parameter $\\epsilon$ to see if it helps the Dyna-Q agent discover the shortcut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_only_cumulative_reward(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_max_steps = exp_parameters['num_max_steps']\n",
    "    epsilons = agent_parameters['epsilons']\n",
    "\n",
    "    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"planning_steps\": agent_parameters[\"planning_steps\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    log_data = {'epsilons' : epsilons} \n",
    "    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))\n",
    "\n",
    "    for eps_idx, epsilon in enumerate(epsilons):\n",
    "\n",
    "        print('Agent : Dyna-Q, epsilon : %f' % epsilon)\n",
    "        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"epsilon\"] = epsilon\n",
    "\n",
    "        for run in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['seed'] = run\n",
    "            agent_info['planning_seed'] = run\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            num_steps = 0\n",
    "            cum_reward = 0\n",
    "\n",
    "            while num_steps < num_max_steps-1 :\n",
    "\n",
    "                rl_glue.rl_start()  # We start the experiment\n",
    "                is_terminal = False\n",
    "\n",
    "                while not is_terminal and num_steps < num_max_steps-1 :\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step and return\n",
    "                    # the reward, and action taken.\n",
    "                    num_steps += 1\n",
    "                    cum_reward += reward\n",
    "                    cum_reward_all[eps_idx][run][num_steps] = cum_reward\n",
    "\n",
    "    log_data['cum_reward_all'] = cum_reward_all\n",
    "    np.save(\"results/Dyna-Q_epsilons\", log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 30,                     # The number of times we run the experiment\n",
    "    \"num_max_steps\" : 6000,              # The number of steps per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "    \"change_at_n\": 3000\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"planning_steps\" : 10,\n",
    "    \"epsilons\": [0.1, 0.2, 0.4, 0.8]    # The list of epsilons we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQAgent              # The agent\n",
    "\n",
    "run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "plot_cumulative_reward('results/Dyna-Q_epsilons.npy', 'epsilons', 'cum_reward_all', 'Cumulative\\nreward', r'$\\epsilon$ = ', r'Dyna-Q : Varying $\\epsilon$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7587c4a396d64d011ff5d8cf0f755f43",
     "grade": false,
     "grade_id": "cell-75b928a3930343ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do you observe?\n",
    "\n",
    "Increasing the exploration via the $\\epsilon$-greedy strategy does not seem to be helping. In fact, the agent's cumulative reward decreases because it is spending more and more time trying out the exploratory actions.\n",
    "\n",
    "Can we do better...? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna-Q+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c201b6bae38cb088c3c3cbc51810c914",
     "grade": false,
     "grade_id": "cell-1ed17a58ff98db6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven't been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.\n",
    "\n",
    "In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \\kappa \\sqrt{ \\tau(s,a)}$, for some small $\\kappa$. \n",
    "\n",
    "Let's implement that!\n",
    "\n",
    "Based on your `DynaQAgent`, create a new class `DynaQPlusAgent` to implement the aforementioned exploration heuristic. Additionally :\n",
    "1. actions that had never been tried before from a state should now be allowed to be considered in the planning step,\n",
    "2. and the initial model for such actions is that they lead back to the same state with a reward of zero.\n",
    "\n",
    "At this point, you might want to refer to the video lectures and [Section 8.3](http://www.incompleteideas.net/book/RLbook2018.pdf#page=188) of the RL textbook for a refresher on Dyna-Q+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b319274d17d4f236253245cbafc2f2c9",
     "grade": false,
     "grade_id": "cell-817a09952176290c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `update_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for update_model() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQPlusAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,0,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,1,2)\n",
    "test_agent.tau[0][0] += 1\n",
    "print(\"Model: \\n\", test_agent.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d5fac20afac86a10733c1eff19544eec",
     "grade": false,
     "grade_id": "cell-7d4bca62495646a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Model: \n",
    " {0: {2: (0, 1), 0: (0, 0), 1: (0, 0), 3: (1, 2)}, 2: {0: (1, 1), 1: (2, 0), 2: (2, 0), 3: (2, 0)}}\n",
    "```\n",
    "Note that the actions that were not taken from a state are also added to the model, with a loop back into the same state with a reward of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d62d53a3b16a0a7fa4842f5775c442c2",
     "grade": false,
     "grade_id": "cell-f03c6dd8052fd06c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `planning_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for planning_step() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"kappa\": 0.001,\n",
    "              \"planning_steps\": 4,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 1}\n",
    "test_agent = DynaQPlusAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,1,-1,1)\n",
    "test_agent.tau += 1; test_agent.tau[0][1] = 0\n",
    "test_agent.update_model(0,2,1,1)\n",
    "test_agent.tau += 1; test_agent.tau[0][2] = 0    # Note that these counts are manually updated \n",
    "test_agent.update_model(2,0,1,1)                 #     as we'll code them in `agent_step'  \n",
    "test_agent.tau += 1; test_agent.tau[2][0] = 0    #     which hasn't been implemented yet.\n",
    "\n",
    "test_agent.planning_step()\n",
    "\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc713c424acb265ebef08b7c5d2321e8",
     "grade": false,
     "grade_id": "cell-c624d442e2ae7d30",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Model: \n",
    " {0: {1: (-1, 1), 0: (0, 0), 2: (1, 1), 3: (0, 0)}, 2: {0: (1, 1), 1: (2, 0), 2: (2, 0), 3: (2, 0)}}\n",
    "Action-value estimates: \n",
    " [[0.         0.10014142 0.         0.        ]\n",
    " [0.         0.         0.         0.        ]\n",
    " [0.         0.00036373 0.         0.00017321]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d0186afc16559c8d05fff29e5b91b50",
     "grade": false,
     "grade_id": "cell-da231fa8a614788e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's test these methods one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "723356966ffc5fbeb16a6cd981071bbe",
     "grade": false,
     "grade_id": "cell-8db85fa89415ea0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `agent_start()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_start() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0,\n",
    "              \"kappa\": 0.001,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQPlusAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "action = test_agent.agent_start(0) # state\n",
    "print(\"Action:\", action)\n",
    "print(\"Timesteps since last visit: \\n\", test_agent.tau)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n",
    "print(\"Model: \\n\", test_agent.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea8c3a78e11dd96c73d1d02933e0ec64",
     "grade": false,
     "grade_id": "cell-f6fb327707c1855c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Action: 1\n",
    "Timesteps since last visit: \n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "Action-value estimates: \n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "Model: \n",
    " {}\n",
    "```\n",
    "Remember the last-visit counts are not updated in `agent_start()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28714342c4f7fec008e01c525e98d2a6",
     "grade": false,
     "grade_id": "cell-be8fc718581879ad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `agent_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_step() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0,\n",
    "              \"kappa\": 0.001,\n",
    "              \"planning_steps\": 4,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQPlusAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions = []\n",
    "actions.append(test_agent.agent_start(0))    # state\n",
    "actions.append(test_agent.agent_step(1,2))   # (reward, state)\n",
    "actions.append(test_agent.agent_step(0,1))   # (reward, state)\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Timesteps since last visit: \\n\", test_agent.tau)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n",
    "print(\"Model: \\n\", test_agent.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7f7f472dbae4f04ab045f9070443158",
     "grade": false,
     "grade_id": "cell-6cd0bcf30529fcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Actions: [1, 3, 1]\n",
    "Timesteps since last visit: \n",
    " [[2. 1. 2. 2.]\n",
    " [2. 2. 2. 2.]\n",
    " [2. 2. 2. 0.]]\n",
    "Action-value estimates: \n",
    " [[1.91000000e-02 2.71000000e-01 0.00000000e+00 1.91000000e-02]\n",
    " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    " [0.00000000e+00 1.83847763e-04 4.24264069e-04 0.00000000e+00]]\n",
    "Model: \n",
    " {0: {1: (2, 1), 0: (0, 0), 2: (0, 0), 3: (0, 0)}, 2: {3: (1, 0), 0: (2, 0), 1: (2, 0), 2: (2, 0)}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "069707871b3203dcaf81c9081726455b",
     "grade": false,
     "grade_id": "cell-ffbeb161866707da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test `agent_end()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code for agent_end() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0,\n",
    "              \"kappa\": 0.001,\n",
    "              \"planning_steps\": 4,\n",
    "              \"seed\": 0,\n",
    "              \"planning_seed\": 0}\n",
    "test_agent = DynaQPlusAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions = []\n",
    "actions.append(test_agent.agent_start(0))\n",
    "actions.append(test_agent.agent_step(1,2))\n",
    "actions.append(test_agent.agent_step(0,1))\n",
    "test_agent.agent_end(1)\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Timesteps since last visit: \\n\", test_agent.tau)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)\n",
    "print(\"Model: \\n\", test_agent.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c008b44213589da78abaf90a07ebb93e",
     "grade": false,
     "grade_id": "cell-e4831f4d1cf10b12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Actions: [1, 3, 1]\n",
    "Timesteps since last visit: \n",
    " [[3. 2. 3. 3.]\n",
    " [3. 0. 3. 3.]\n",
    " [3. 3. 3. 1.]]\n",
    "Action-value estimates: \n",
    " [[1.91000000e-02 3.44083848e-01 0.00000000e+00 4.44632051e-02]\n",
    " [1.91732051e-02 1.90000000e-01 0.00000000e+00 0.00000000e+00]\n",
    " [0.00000000e+00 1.83847763e-04 4.24264069e-04 0.00000000e+00]]\n",
    "Model: \n",
    " {0: {1: (2, 1), 0: (0, 0), 2: (0, 0), 3: (0, 0)}, 2: {3: (1, 0), 0: (2, 0), 1: (2, 0), 2: (2, 0)}, 1: {1: (-1, 1), 0: (1, 0), 2: (1, 0), 3: (1, 0)}}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "334f1ceca4ab090a3e1ec653e3b496c2",
     "grade": false,
     "grade_id": "cell-839b7e5d8b7c439f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Experiment: Dyna-Q+ agent in the _changing_ environment\n",
    "\n",
    "Okay, now we're ready to test our Dyna-Q+ agent on the Shortcut Maze. As usual, we will average the results over 30 independent runs of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT modify the parameter settings.\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 30,                     # The number of times we run the experiment\n",
    "    \"num_max_steps\" : 6000,              # The number of steps per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "    \"change_at_n\": 3000\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"step_size\" : 0.5,\n",
    "    \"planning_steps\" : [50]      \n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQPlusAgent          # The agent\n",
    "\n",
    "run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, \"Dyna-Q+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0aae91605b0be56ef43e821b1719bfbf",
     "grade": false,
     "grade_id": "cell-d1f8fd21d4357f1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's compare the Dyna-Q and Dyna-Q+ agents with `planning_steps=50` each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_reward_comparison(file_name_dynaq, file_name_dynaqplus):\n",
    "\n",
    "    cum_reward_q = np.load(file_name_dynaq, allow_pickle=True).item()['cum_reward_all'][2]\n",
    "    cum_reward_qPlus = np.load(file_name_dynaqplus, allow_pickle=True).item()['cum_reward_all'][0]\n",
    "\n",
    "    plt.plot(np.mean(cum_reward_qPlus, axis=0), label='Dyna-Q+')\n",
    "    plt.plot(np.mean(cum_reward_q, axis=0), label='Dyna-Q')\n",
    "\n",
    "    plt.axvline(x=3000, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Cumulative\\nreward', rotation=0, labelpad=60)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\\n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_reward_comparison('results/Dyna-Q_shortcut_steps.npy', 'results/Dyna-Q+.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21076543e2048b8680e6e0aef7fe3eb8",
     "grade": false,
     "grade_id": "cell-1754be7eabd4a8aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do you observe? (For reference, your graph should look like [Figure 8.5 in Chapter 8](http://www.incompleteideas.net/book/RLbook2018.pdf#page=189) of the RL textbook)\n",
    "\n",
    "The slope of the curve increases for the Dyna-Q+ curve shortly after the shortcut opens up after 3000 steps, which indicates that the rate of receiving the positive reward increases. This implies that the Dyna-Q+ agent finds the shorter path to the goal.\n",
    "\n",
    "To verify this, let us plot the state-visitations of the Dyna-Q+ agent before and after the shortcut opens up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_visitations(\"results/Dyna-Q+.npy\", ['Dyna-Q+ : State visitations before the env changes', 'Dyna-Q+ : State visitations after the env changes'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "449ce5338414248561fc1c8219cc3792",
     "grade": false,
     "grade_id": "cell-fafb7f5a25d136fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do you observe?\n",
    "\n",
    "Before the shortcut opens up, like Dyna-Q, the Dyna-Q+ agent finds the sole, long path to the goal. But because the Dyna-Q+ agent keeps exploring, it succeeds in discovering the shortcut once it opens up, which leads to the goal faster. So the bonus reward heuristic is effective in helping the agent explore and find changes in the environment without degrading the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "29cd9402feeb69c6d64d8ae08c9aa201",
     "grade": false,
     "grade_id": "cell-89485f6ff67b4a48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Congratulations! You have:\n",
    "\n",
    "1. implemented Dyna-Q, a model-based approach to RL;\n",
    "2. implemented Dyna-Q+, a variant of Dyna-Q with an exploration bonus that encourages exploration; \n",
    "3. conducted scientific experiments to empirically validate the exploration/exploitation dilemma in the planning context on an environment that changes with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points to ponder about:\n",
    "1. At what cost does Dyna-Q+ improve over Dyna-Q?\n",
    "2. In general, what is the trade-off of using model-based methods like Dyna-Q over model-free methods like Q-learning?"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "sample-based-learning-methods",
   "graded_item_id": "trR7Z",
   "launcher_item_id": "edrCE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
