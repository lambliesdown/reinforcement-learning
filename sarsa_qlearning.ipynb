{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning and Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Course 2 Programming Assignment 2. In this notebook, you will:\n",
    "\n",
    "- Implement Q-Learning with $\\epsilon$-greedy action selection\n",
    "- Implement Expected Sarsa with $\\epsilon$-greedy action selection\n",
    "- Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a6df636f47ebdf7f0707d7b2651a2c6",
     "grade": false,
     "grade_id": "cell-2a8ddbbf0ef25d07",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d74b7bc264a49057450f81177d1afbdb",
     "grade": false,
     "grade_id": "cell-69f08c6441da699c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will need the following libraries for this assignment. We are using:\n",
    "1. numpy: the fundamental package for scientific computing with Python.\n",
    "2. scipy: a Python library for scientific and technical computing.\n",
    "3. matplotlib: library for plotting graphs in Python.\n",
    "4. RL-Glue: library for reinforcement learning experiments.\n",
    "\n",
    "**Please do not import other libraries** â€” this will break the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from rllib.agent.q_learning_agent import QLearningAgent\n",
    "from rllib.agent.expected_sarsa_agent import ExpectedSarsaAgent\n",
    "from rllib.environment.cliff_walk_environment import CliffWalkEnvironment\n",
    "from rllib.rl_glue import RLGlue\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "781be58c941d2ddc62052efda26ebd05",
     "grade": false,
     "grade_id": "cell-92144e79fff2c0ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [10,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4869e937cb5c63d7046a204ebe15914c",
     "grade": false,
     "grade_id": "cell-0c942413e94d98db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this section you will implement and test a Q-Learning agent with $\\epsilon$-greedy action selection (Section 6.5 in the textbook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9361d06fd03ef5169c039e916de4ec26",
     "grade": false,
     "grade_id": "cell-5bb232d570f6ba80",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "301cb73e95ae17680f0d24e10c7513d6",
     "grade": false,
     "grade_id": "cell-d2621de8f8b5e4ba",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cells below to test the implemented methods. The output of each cell should match the expected output.\n",
    "\n",
    "Note that passing this test does not guarantee correct behavior on the Cliff World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_start() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = QLearningAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "action = current_agent.agent_start(0)\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fbe3f4201266f67423b1ece02dbc0333",
     "grade": false,
     "grade_id": "cell-f1a6a8b66b6598e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "Action: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_step() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = QLearningAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "actions.append(current_agent.agent_start(0))\n",
    "actions.append(current_agent.agent_step(2, 1))\n",
    "actions.append(current_agent.agent_step(0, 0))\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed1a688d14e6eb3961b32a8dbdbbb858",
     "grade": false,
     "grade_id": "cell-3b916a9081886d4d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[ 0.   0.2  0.   0. ]\n",
    " [ 0.   0.   0.  0.02]\n",
    " [ 0.   0.   0.   0. ]]\n",
    "Actions: [1, 3, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_end() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = QLearningAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "actions.append(current_agent.agent_start(0))\n",
    "actions.append(current_agent.agent_step(2, 1))\n",
    "current_agent.agent_end(1)\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d34093b01b729874834af87668416b5f",
     "grade": false,
     "grade_id": "cell-8eddb10c5e7c1791",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[0.  0.2 0.  0. ]\n",
    " [0.  0.  0.  0.1]\n",
    " [0.  0.  0.  0. ]]\n",
    "Actions: [1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a549cc5d3d6a35b2578be87a3ea288a",
     "grade": false,
     "grade_id": "cell-3ab82a89ea44f09e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 2:  Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e16e2e0918866de0908360b07d53b814",
     "grade": false,
     "grade_id": "cell-12980d9f811d7bb6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this section you will implement an Expected Sarsa agent with $\\epsilon$-greedy action selection (Section 6.6 in the textbook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f358f7e2676a77b8dd13a09fad9261a2",
     "grade": false,
     "grade_id": "cell-bd6580041d80533a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "562af8b2c4449bec9534666c9747e461",
     "grade": false,
     "grade_id": "cell-7574736a2553024d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cells below to test the implemented methods. The output of each cell should match the expected output.\n",
    "\n",
    "Note that passing this test does not guarantee correct behavior on the Cliff World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_start() ##\n",
    "\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = ExpectedSarsaAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "action = current_agent.agent_start(0)\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f5cc33e33a94e5123e0311be2208c2a",
     "grade": false,
     "grade_id": "cell-4d1ae44ff39f2ef6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "Action: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_step() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = ExpectedSarsaAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "actions.append(current_agent.agent_start(0))\n",
    "actions.append(current_agent.agent_step(2, 1))\n",
    "actions.append(current_agent.agent_step(0, 0))\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e92c78b348a88e1db2e988fd442a1ae5",
     "grade": false,
     "grade_id": "cell-11bdb20cca21c6d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[0.     0.2    0.     0.    ]\n",
    " [0.     0.     0.     0.0185]\n",
    " [0.     0.     0.     0.    ]]\n",
    "Actions: [1, 3, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code for agent_end() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "current_agent = ExpectedSarsaAgent()\n",
    "current_agent.agent_init(agent_info)\n",
    "actions.append(current_agent.agent_start(0))\n",
    "actions.append(current_agent.agent_step(2, 1))\n",
    "current_agent.agent_end(1)\n",
    "print(\"Action Value Estimates: \\n\", current_agent.q)\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9a2554acf9aa8d280d1175c3f23554b",
     "grade": false,
     "grade_id": "cell-9edd1b6d5a51c18a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Action Value Estimates: \n",
    " [[0.  0.2 0.  0. ]\n",
    " [0.  0.  0.  0.1]\n",
    " [0.  0.  0.  0. ]]\n",
    "Actions: [1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Cliff World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f6c1e54b358fabad02c9002f23a1087",
     "grade": false,
     "grade_id": "cell-6e7fbbaa12d4bf31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We described the Cliff World environment in the video \"Expected Sarsa in the Cliff World\" in Lesson 3. This is an undiscounted episodic task and thus we set $\\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cliffworld.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4215fbaa30c33d57f4351e501f0a6422",
     "grade": false,
     "grade_id": "cell-e55d077b9f8b6133",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.\n",
    "\n",
    "The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    agents = {\n",
    "        \"Q-learning\": QLearningAgent,\n",
    "        \"Expected Sarsa\": ExpectedSarsaAgent\n",
    "    }\n",
    "    env = CliffWalkEnvironment #cliffworld_env.Environment\n",
    "    all_reward_sums = {} # Contains sum of rewards during episode\n",
    "    all_state_visits = {} # Contains state visit counts during the last 10 episodes\n",
    "    agent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"step_size\": 0.5, \"discount\": 1.0}\n",
    "    env_info = {}\n",
    "    num_runs = 100 # The number of runs\n",
    "    num_episodes = 500 # The number of episodes in each run\n",
    "    \n",
    "    for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "        all_reward_sums[algorithm] = []\n",
    "        all_state_visits[algorithm] = []\n",
    "        for run in tqdm(range(num_runs)):\n",
    "            agent_info[\"seed\"] = run\n",
    "            rl_glue = RLGlue(env, agents[algorithm])\n",
    "            rl_glue.rl_init(agent_info, env_info)\n",
    "    \n",
    "            reward_sums = []\n",
    "            state_visits = np.zeros(48)\n",
    "            for episode in range(num_episodes):\n",
    "                if episode < num_episodes - 10:\n",
    "                    # Runs an episode\n",
    "                    rl_glue.rl_episode(0) \n",
    "                else: \n",
    "                    # Runs an episode while keeping track of visited states\n",
    "                    state, action = rl_glue.rl_start()\n",
    "                    state_visits[state] += 1\n",
    "                    is_terminal = False\n",
    "                    while not is_terminal:\n",
    "                        reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                        state_visits[state] += 1\n",
    "                    \n",
    "                reward_sums.append(rl_glue.rl_return())\n",
    "                \n",
    "            all_reward_sums[algorithm].append(reward_sums)\n",
    "            all_state_visits[algorithm].append(state_visits)\n",
    "    \n",
    "\n",
    "    for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "        plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "    plt.xlim(0,500)\n",
    "    plt.ylim(-100,0)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1cbb34897b56a32ea1e378b95caa0842",
     "grade": false,
     "grade_id": "cell-c3967df7d24c7d02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To see why these two agents behave differently, let's inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for algorithm, position in [(\"Q-learning\", 211), (\"Expected Sarsa\", 212)]:\n",
    "        plt.subplot(position)\n",
    "        average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=0)\n",
    "        # CliffWalkEnvironment is upside down vs. cliffworld_env\n",
    "        grid_state_visits = np.flipud(average_state_visits.reshape((4,12)))\n",
    "        grid_state_visits[0,1:-1] = np.nan\n",
    "        plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=2)\n",
    "        plt.title(algorithm)\n",
    "        plt.axis('off')\n",
    "        cm = plt.get_cmap()\n",
    "        cm.set_bad('gray')\n",
    "    \n",
    "        plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n",
    "        cax = plt.axes([0.85, 0.0, 0.075, 1.])\n",
    "        \n",
    "    cbar = plt.colorbar(cax=cax)\n",
    "    cbar.ax.set_ylabel(\"Visits during\\n the last 10\\n episodes\", rotation=0, labelpad=70)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e20aaec2eb1806cda6de9f75002264d5",
     "grade": false,
     "grade_id": "cell-c7575e40e56f751c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path. Note this is different from the book. The book shows Sarsa learns the even safer path\n",
    "\n",
    "\n",
    "Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?\n",
    "\n",
    "In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.\n",
    "\n",
    "This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "agents = {\n",
    "    \"Q-learning\": QLearningAgent,\n",
    "    \"Expected Sarsa\": ExpectedSarsaAgent\n",
    "}\n",
    "env = CliffWalkEnvironment\n",
    "all_reward_sums = {}\n",
    "step_sizes = np.linspace(0.1,1.0,10)\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"discount\": 1.0}\n",
    "env_info = {}\n",
    "num_runs = 100\n",
    "num_episodes = 100\n",
    "all_reward_sums = {}\n",
    "\n",
    "for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "    for step_size in step_sizes:\n",
    "        all_reward_sums[(algorithm, step_size)] = []\n",
    "        agent_info[\"step_size\"] = step_size\n",
    "        for run in tqdm(range(num_runs)):\n",
    "            agent_info[\"seed\"] = run\n",
    "            rl_glue = RLGlue(env, agents[algorithm])\n",
    "            rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "            return_sum = 0\n",
    "            for episode in range(num_episodes):\n",
    "                rl_glue.rl_episode(0)\n",
    "                return_sum += rl_glue.rl_return()\n",
    "            all_reward_sums[(algorithm, step_size)].append(return_sum/num_episodes)\n",
    "        \n",
    "\n",
    "for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n",
    "    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n",
    "    plt.plot(step_sizes, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n",
    "    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Step-size\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\n",
    "plt.xticks(step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6113751690c166257cd1ace47ef977b1",
     "grade": false,
     "grade_id": "cell-e2c9c37b494e40f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "685455388a3911be0ef17e3d53445862",
     "grade": false,
     "grade_id": "cell-10150ffd5c7c91f8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.\n",
    "\n",
    "Congratulations! Now you have:\n",
    "\n",
    "- implemented Q-Learning with $\\epsilon$-greedy action selection\n",
    "- implemented Expected Sarsa with $\\epsilon$-greedy action selection\n",
    "- investigated the behavior of these two algorithms on Cliff World"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "sample-based-learning-methods",
   "graded_item_id": "ofXCB",
   "launcher_item_id": "biN1L"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
